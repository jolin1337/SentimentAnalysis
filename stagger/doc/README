
                    Stagger -- The Stockholm Tagger


    Introduction
    ------------

Stagger is a part of speech tagger and named entity recognizer. Though
originally designed for Swedish, it has has also been tested with English
and Chinese.

The tagger was written by Robert Ã–stling (robert@ling.su.se), and is
distributed under the GNU GPL licence (see the LICENSE file in this
directory).

For further information about Stagger, please visit the Stockholm
University website:

  http://www.ling.su.se/stagger


    Compiling
    ---------

NOTE: use the precompiled stagger.jar file if you are only interested in
using the tagger. Skip to the next section, "Quick start", to learn how to use
stagger.jar.

The rest of this section concerns building (compiling) Stagger, in case you
have changed the source code and want to use the modified version. Most users
will not need this.

First, install JFlex (http://www.jflex.de), Java Development Kit (JDK) and
Apache Ant.

Then, simply type "ant" in the stagger directory.


    Quick start
    -----------

To train a plain tagger from the Penn Treebank data:

    java -Xmx8G -ea -jar stagger.jar -devfile ptb/dev.conll \
        -trainfile ptb/train.conll -modelfile models/ptb.bin -lang en -train

To tag a text document with the model created above:

    java -jar stagger.jar -modelfile models/ptb.bin -tag test.txt >test.conll


    File formats
    ------------

Stagger supports three file formats:

* Plain format, with one token/tag pair per line on the format
  "token<space(s)>tag", and blank lines after sentences. This is the default
  input format unless the file ends with ".conll" or ".txt".
  For output, use the -plain option to enable plain format.

* CoNLL-style format, see the FORMAT file for further documentation, and
  perhaps the SIC corpus (http://www.ling.su.se/sic) if you need an example.

* Raw text to be tokenized (input only), this is used for filenames ending
  with ".txt"


    Command-line options
    --------------------

    -lexicon <filename>

For training, an optional POS lexicon may be specified. Each line in the given
file should have these 4 tab-separated fields:

    word form       citation form       POS tag         frequency

Ambiguous word forms have one line for each possible POS tag.
The frequency field is typically 0 when taken from a dictionary, but can be
non-zero if derived from a POS-tagged lexicon.
NOTE: this dictionary is automatically extended by the training set before
training, and if no dictionary is available this option can be left out.

    -dict <pos | ne | all> <filename.dict>

Use a word class dictionary for POS tagging (pos), Named Entity Recognition
(ne) or both (all). Each line in the file contains two tab-separated fields:
a word form, and a word class. Both are strings. The word class could be e.g.
a Brown cluster identifier. Each word form should occur only once.

    -embed <pos | ne | all> <filename.embed>

Similar to the above, but the format is:

    word form       x_1     x_2     x_3     ...

Where each x_i is a real-valued number, representing the i:th dimension of the
given word form. This can be used for word representations, see:
http://metaoptimize.com/projects/wordreprs/

    -lang <language code>

Currently "sv", "en", "is", "zh" and "any" are supported. The "any" option
uses a generic Latin tokenizer, with a generic tagger class that considers
the entire tagset for unknown words, reducing accuracy and performance.
This argument is mandatory for training.

    -trainfile <filename>

File to use as training set.

    -devfile <filename>

File to use as development set, whose only use is to determine when to
abort the training. If no development set is provided, the -positers and
-neiters arguments determine the number of training iterations. A value of 0
disables the given tagger entirely.

    -noner

Disable NER (in tagging mode).

    -positers <n>       -neiters <n>

Train the POS tagger/NER with at most n iterations. If the -devfile option is
used, training may be aborted earlier than this if the accuracy peaks.
Default values are 16 for both.

    -posbeamsize <n>    -nebeamsize <n>

Use the beam size n for the POS tagger/NER. Defaults are 8 and 4.
A small amount of accuracy can be gained by increasing these, at the expense
of performance, and vice versa.

    -modelfile <filename.bin>

Model file to use, for training this will be written to, when tagging it is
read.

    -train

Perform training. This should be the last argument.

    -preserve

Useful when tagging partially tagged .conll files, to not overwrite existing
annotation, in case one wants to e.g. only do NER but leave POS tags intact.

    -plain

Output plain data, with each line on the format "token<tab>tag" and blank
lines after sentences. All other information is discarded.
If used with -tokenize, no tags are output, but only one token per line with
blank lines after sentences.

    -tag <filename.conll | filename.txt | filename.*> [filename2 [...]]

Tag the given file, and write the result to stdout. If the filename ends with
something other than ".txt", an evaluation will also be performed, so in these
cases it makes sense to redirect stdout to /dev/null.
If the filename ends with ".txt", raw text is assumed, and the relevant
tokenizer is invoked.
If more than one file is specified, the output is written to a file with the
same name as each input, but with a .conll or .plain suffix (depending on
whether or not the -plain option is used).
NOTE: there is currently no Chinese tokenizer.

    -tokenize <filename>

Tokenize the given file, using the tokenizer specified by the -lang option,
and write the output (space-separated tokens, one line per sentence) to stdout.
If -plain is used, write one token per line, with blank lines after sentences.


    Preparing the lexicon
    ---------------------

There is a script, lexicon.py, which can create a lexicon file from the SALDO
morphology database, and (optionally) lists of foreign words and proper nouns.

    scripts/lexicon.py saldom.xml UO:foreign.txt PM:names.txt >lexicon.txt

The foreign words and proper noun lists should contain one entry per line.
Anything after a tab character will be ignored, so embedding files (see the
option -embed to the tagger above) may be used for this purpose.


    Evaluations
    -----------

There are two scripts to assist in evaluations, particularly cross-validations.
They have been tested with the SUC corpus, but should be usable or easily
adaptable to other corpora.

See the files cross.sh and stats.py in the scripts directory for further
information. Features include known/unknown word accuracy, tag confusion
statistics, and McNemar's test.

